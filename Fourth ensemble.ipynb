{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 670 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.layers import LeakyReLU, PReLU, ELU, ParametricSoftplus, ThresholdedLinear, ThresholdedReLU, SReLU\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.regularizers import l1, l2, l1l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "  'good_for_lunch', \n",
    "  'good_for_dinner', \n",
    "  'takes_reservations', \n",
    "  'outdoor_seating',\n",
    "  'restaurant_is_expensive',\n",
    "  'has_alcohol',\n",
    "  'has_table_service',\n",
    "  'ambience_is_classy',\n",
    "  'good_for_kids'\n",
    "]\n",
    "\n",
    "vgg_cols = ['f' + str(i) for i in range(4096)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1596, 4096) (400, 4096) (1596, 9) (400, 9)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_hdf('data/Xtrain_grouped.hdf5')\n",
    "X, Y = np.array(data[vgg_cols]), np.array(data[categories])\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X, Y, test_size=.2, random_state=random_state)\n",
    "valset = (Xval, yval)\n",
    "\n",
    "print Xtrain.shape, Xval.shape, ytrain.shape, yval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1637\n",
      "2 1707\n",
      "3 1654\n",
      "4 1746\n",
      "5 1735\n",
      "6 1521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 4096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([(0,1,2,3,4,5,6,7,8)])\n",
    "\n",
    "dataTest = None\n",
    "for i in range(1, 7):\n",
    "  part = pd.read_hdf('data/Xtest_grouped_part' + str(i) + '.hdf5', 'Xtest')\n",
    "  print i, len(part)\n",
    "  \n",
    "  if dataTest is None:\n",
    "    dataTest = part\n",
    "  else:\n",
    "    dataTest = dataTest.append(part)\n",
    "    \n",
    "Xtest = np.array(dataTest[vgg_cols])\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(ytrue, ypred):\n",
    "  return K.sum( (ytrue - ypred) ** 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class f1stopperCallback(Callback):\n",
    "  def __init__(self):\n",
    "    self.bestf1val = 0\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    pred = self.model.predict(Xval)\n",
    "    pred[pred < .5] = 0\n",
    "    pred[pred >= .5] = 1\n",
    "    f1_val = f1_score(yval, pred, average='micro')\n",
    "\n",
    "    if f1_val > self.bestf1val:\n",
    "      self.bestf1val = f1_val\n",
    "      self.model.save_weights('models/tmp_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700]\n",
      "[ 0.1   0.15  0.2   0.25  0.3   0.35  0.4   0.45  0.5 ]\n"
     ]
    }
   ],
   "source": [
    "neurons = range(50, 701, 50)\n",
    "dropout_vals = np.linspace(.1, .5, 9)\n",
    "models = []\n",
    "print neurons\n",
    "print dropout_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-layer nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_1_layer_model(n_neurons, dropout_val):\n",
    "  model = Sequential()\n",
    "  np.random.seed(0)\n",
    "  \n",
    "  model.add(Dense(n_neurons, input_shape=(4096,), activation='softplus'))\n",
    "  model.add(Dropout(dropout_val))\n",
    "\n",
    "  model.add(Dense(9, activation='sigmoid'))\n",
    "  model.compile(loss=loss, optimizer='adam')\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.1 0.824536376605 0.839407744875\n",
      "50 0.15 0.826267664173 0.840364880274\n",
      "50 0.2 0.825636719843 0.839831697055\n",
      "50 0.25 0.829988851728 0.839215686275\n",
      "50 0.3 0.82888004548 0.841687907108\n",
      "50 0.35 0.828456683878 0.839540744889\n",
      "50 0.4 0.828988828416 0.840915512857\n",
      "50 0.45 0.833666191155 0.837799717913\n",
      "50 0.5 0.830654420207 0.837592277115\n",
      "100 0.1 0.824724809483 0.837446808511\n",
      "100 0.15 0.826285714286 0.841184767278\n",
      "100 0.2 0.827861832715 0.841865756542\n",
      "100 0.25 0.832050185344 0.840993434199\n",
      "100 0.3 0.831139095592 0.839021615472\n",
      "100 0.35 0.831146605819 0.842193153354\n",
      "100 0.4 0.828596037898 0.841782729805\n",
      "100 0.45 0.836209335219 0.841838173104\n",
      "100 0.5 0.840750069969 0.841927303466\n",
      "150 0.1 0.825231481481 0.837276399308\n",
      "150 0.15 0.826989619377 0.839829302987\n",
      "150 0.2 0.828439838803 0.840157038699\n",
      "150 0.25 0.829128440367 0.841779279279\n",
      "150 0.3 0.834272829763 0.841722595078\n",
      "150 0.35 0.823529411765 0.840283687943\n",
      "150 0.4 0.8342760181 0.842706251784\n",
      "150 0.45 0.833238474673 0.842283776145\n",
      "150 0.5 0.837510560406 0.84246188594\n",
      "200 0.1 0.825297787862 0.83917640512\n",
      "200 0.15 0.821034282394 0.838255977496\n",
      "200 0.2 0.826326471441 0.839582745983\n",
      "200 0.25 0.821659799882 0.842425920798\n",
      "200 0.3 0.82516671499 0.843907916901\n",
      "200 0.35 0.827526132404 0.843654540405\n",
      "200 0.4 0.835273573923 0.842017275007\n",
      "200 0.45 0.82788798133 0.843510366373\n",
      "200 0.5 0.831092928112 0.843475799604\n",
      "250 0.1 0.818689391713 0.838111298482\n",
      "250 0.15 0.824076766502 0.840364880274\n",
      "250 0.2 0.820421490056 0.84007925276\n",
      "250 0.25 0.82959124928 0.842370039128\n",
      "250 0.3 0.8233559422 0.841387151791\n",
      "250 0.35 0.83134284882 0.840369024322\n",
      "250 0.4 0.830122591944 0.842885657257\n",
      "250 0.45 0.833381797034 0.842105263158\n",
      "250 0.5 0.831146106737 0.845094180489\n",
      "300 0.1 0.819472616633 0.83949930459\n",
      "300 0.15 0.821980538065 0.842395804582\n",
      "300 0.2 0.825174825175 0.838546069315\n",
      "300 0.25 0.827859569649 0.840507726269\n",
      "300 0.3 0.827367205543 0.842374325476\n",
      "300 0.35 0.8345567476 0.841274316324\n",
      "300 0.4 0.826312752452 0.841575859179\n",
      "300 0.45 0.826812428078 0.840993434199\n",
      "300 0.5 0.829084588644 0.841748942172\n",
      "350 0.1 0.828947368421 0.844383561644\n",
      "350 0.15 0.826751225137 0.841722595078\n",
      "350 0.2 0.828018223235 0.839966601726\n",
      "350 0.25 0.833854018745 0.842820730671\n",
      "350 0.3 0.834973943254 0.845852017937\n",
      "350 0.35 0.833574529667 0.841466776951\n",
      "350 0.4 0.828395423878 0.843803056027\n",
      "350 0.45 0.824427480916 0.845939933259\n",
      "350 0.5 0.830439814815 0.843713956171\n",
      "400 0.1 0.828985507246 0.842896935933\n",
      "400 0.15 0.82194979568 0.839038189533\n",
      "400 0.2 0.827666376053 0.84127874369\n",
      "400 0.25 0.815245853942 0.840660879306\n",
      "400 0.3 0.823632130384 0.841206602163\n",
      "400 0.35 0.823939570017 0.843946433452\n",
      "400 0.4 0.828943560058 0.842721695482\n",
      "400 0.45 0.832179930796 0.843176038429\n",
      "400 0.5 0.832947976879 0.844910688971\n",
      "450 0.1 0.824006958539 0.840839160839\n",
      "450 0.15 0.826635376943 0.840915512857\n",
      "450 0.2 0.832283915283 0.839896521989\n",
      "450 0.25 0.826441467676 0.840331333905\n",
      "450 0.3 0.82945067587 0.842574533296\n",
      "450 0.35 0.828497558173 0.843120567376\n",
      "450 0.4 0.830503597122 0.840563380282\n",
      "450 0.45 0.826200873362 0.842499280161\n",
      "450 0.5 0.82818365579 0.846590909091\n",
      "500 0.1 0.817971014493 0.837631056957\n",
      "500 0.15 0.820437956204 0.840942134341\n",
      "500 0.2 0.82102189781 0.840876743524\n",
      "500 0.25 0.831700288184 0.842683632157\n",
      "500 0.3 0.822451994092 0.842543737851\n",
      "500 0.35 0.825806451613 0.84225431889\n",
      "500 0.4 0.83044982699 0.844457175594\n",
      "500 0.45 0.826760151914 0.842979942693\n",
      "500 0.5 0.834470989761 0.843353090601\n",
      "550 0.1 0.82428115016 0.839561674628\n",
      "550 0.15 0.820512820513 0.83775481709\n",
      "550 0.2 0.820046893318 0.840312674484\n",
      "550 0.25 0.821491485614 0.839244851259\n",
      "550 0.3 0.816180844735 0.840786100826\n",
      "550 0.35 0.827327760161 0.842017275007\n",
      "550 0.4 0.825125331761 0.844668345928\n",
      "550 0.45 0.829212190914 0.842165568605\n",
      "550 0.5 0.834851936219 0.841206602163\n",
      "600 0.1 0.831977559607 0.838202247191\n",
      "600 0.15"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "('Error allocating 9830400 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e9e0f7492142>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mdro\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdropout_vals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mneu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdro\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_1_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf1stopperCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-5e9d58e18bc5>\u001b[0m in \u001b[0;36mget_1_layer_model\u001b[1;34m(n_neurons, dropout_val)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, class_mode, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m         updates = self.optimizer.get_updates(self.trainable_weights,\n\u001b[0;32m    549\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m                                              train_loss)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[0mupdates\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/optimizers.pyc\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, params, constraints, loss)\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;31m# zero init of velocity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[0mm_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mvariable\u001b[1;34m(value, dtype, name)\u001b[0m\n\u001b[0;32m     34\u001b[0m     '''\n\u001b[0;32m     35\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/theano/compile/sharedvalue.pyc\u001b[0m in \u001b[0;36mshared\u001b[1;34m(value, name, strict, allow_downcast, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                 var = ctor(value, name=name, strict=strict,\n\u001b[1;32m--> 247\u001b[1;33m                            allow_downcast=allow_downcast, **kwargs)\n\u001b[0m\u001b[0;32m    248\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/var.pyc\u001b[0m in \u001b[0;36mfloat32_shared_constructor\u001b[1;34m(value, name, strict, allow_downcast, borrow, broadcastable, target)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;31m# type.broadcastable is guaranteed to be a tuple, which this next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# function requires\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mdeviceval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_support_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcastable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: ('Error allocating 9830400 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")"
     ]
    }
   ],
   "source": [
    "for neu in neurons:\n",
    "  for dro in dropout_vals:\n",
    "    print neu, dro,\n",
    "    model = get_1_layer_model(neu, dro)\n",
    "    \n",
    "    model.fit(Xtrain, ytrain, verbose=0, callbacks=[f1stopperCallback()], nb_epoch=200)\n",
    "    \n",
    "    pred = model.predict(Xval)\n",
    "    pred[pred < .5] = 0\n",
    "    pred[pred >= .5] = 1\n",
    "    f1_val_last = f1_score(yval, pred, average='micro')\n",
    "    \n",
    "    model.load_weights('models/tmp_weights.h5')\n",
    "    \n",
    "    pred = model.predict(Xval)\n",
    "    pred[pred < .5] = 0\n",
    "    pred[pred >= .5] = 1\n",
    "    f1_val_best = f1_score(yval, pred, average='micro')\n",
    "    \n",
    "    print f1_val_last, f1_val_best\n",
    "    \n",
    "    models.append((f1_val_best, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(450.0, 0.5, 0.82818365579, 0.846590909091),\n",
       " (350.0, 0.45, 0.824427480916, 0.845939933259),\n",
       " (350.0, 0.3, 0.834973943254, 0.845852017937),\n",
       " (250.0, 0.5, 0.831146106737, 0.845094180489),\n",
       " (400.0, 0.5, 0.832947976879, 0.844910688971),\n",
       " (550.0, 0.4, 0.825125331761, 0.844668345928),\n",
       " (500.0, 0.4, 0.83044982699, 0.844457175594),\n",
       " (350.0, 0.1, 0.828947368421, 0.844383561644)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev = \"\\\n",
    "50 0.1 0.824536376605 0.839407744875q\\\n",
    "50 0.15 0.826267664173 0.840364880274q\\\n",
    "50 0.2 0.825636719843 0.839831697055q\\\n",
    "50 0.25 0.829988851728 0.839215686275q\\\n",
    "50 0.3 0.82888004548 0.841687907108q\\\n",
    "50 0.35 0.828456683878 0.839540744889q\\\n",
    "50 0.4 0.828988828416 0.840915512857q\\\n",
    "50 0.45 0.833666191155 0.837799717913q\\\n",
    "50 0.5 0.830654420207 0.837592277115q\\\n",
    "100 0.1 0.824724809483 0.837446808511q\\\n",
    "100 0.15 0.826285714286 0.841184767278q\\\n",
    "100 0.2 0.827861832715 0.841865756542q\\\n",
    "100 0.25 0.832050185344 0.840993434199q\\\n",
    "100 0.3 0.831139095592 0.839021615472q\\\n",
    "100 0.35 0.831146605819 0.842193153354q\\\n",
    "100 0.4 0.828596037898 0.841782729805q\\\n",
    "100 0.45 0.836209335219 0.841838173104q\\\n",
    "100 0.5 0.840750069969 0.841927303466q\\\n",
    "150 0.1 0.825231481481 0.837276399308q\\\n",
    "150 0.15 0.826989619377 0.839829302987q\\\n",
    "150 0.2 0.828439838803 0.840157038699q\\\n",
    "150 0.25 0.829128440367 0.841779279279q\\\n",
    "150 0.3 0.834272829763 0.841722595078q\\\n",
    "150 0.35 0.823529411765 0.840283687943q\\\n",
    "150 0.4 0.8342760181 0.842706251784q\\\n",
    "150 0.45 0.833238474673 0.842283776145q\\\n",
    "150 0.5 0.837510560406 0.84246188594q\\\n",
    "200 0.1 0.825297787862 0.83917640512q\\\n",
    "200 0.15 0.821034282394 0.838255977496q\\\n",
    "200 0.2 0.826326471441 0.839582745983q\\\n",
    "200 0.25 0.821659799882 0.842425920798q\\\n",
    "200 0.3 0.82516671499 0.843907916901q\\\n",
    "200 0.35 0.827526132404 0.843654540405q\\\n",
    "200 0.4 0.835273573923 0.842017275007q\\\n",
    "200 0.45 0.82788798133 0.843510366373q\\\n",
    "200 0.5 0.831092928112 0.843475799604q\\\n",
    "250 0.1 0.818689391713 0.838111298482q\\\n",
    "250 0.15 0.824076766502 0.840364880274q\\\n",
    "250 0.2 0.820421490056 0.84007925276q\\\n",
    "250 0.25 0.82959124928 0.842370039128q\\\n",
    "250 0.3 0.8233559422 0.841387151791q\\\n",
    "250 0.35 0.83134284882 0.840369024322q\\\n",
    "250 0.4 0.830122591944 0.842885657257q\\\n",
    "250 0.45 0.833381797034 0.842105263158q\\\n",
    "250 0.5 0.831146106737 0.845094180489q\\\n",
    "300 0.1 0.819472616633 0.83949930459q\\\n",
    "300 0.15 0.821980538065 0.842395804582q\\\n",
    "300 0.2 0.825174825175 0.838546069315q\\\n",
    "300 0.25 0.827859569649 0.840507726269q\\\n",
    "300 0.3 0.827367205543 0.842374325476q\\\n",
    "300 0.35 0.8345567476 0.841274316324q\\\n",
    "300 0.4 0.826312752452 0.841575859179q\\\n",
    "300 0.45 0.826812428078 0.840993434199q\\\n",
    "300 0.5 0.829084588644 0.841748942172q\\\n",
    "350 0.1 0.828947368421 0.844383561644q\\\n",
    "350 0.15 0.826751225137 0.841722595078q\\\n",
    "350 0.2 0.828018223235 0.839966601726q\\\n",
    "350 0.25 0.833854018745 0.842820730671q\\\n",
    "350 0.3 0.834973943254 0.845852017937q\\\n",
    "350 0.35 0.833574529667 0.841466776951q\\\n",
    "350 0.4 0.828395423878 0.843803056027q\\\n",
    "350 0.45 0.824427480916 0.845939933259q\\\n",
    "350 0.5 0.830439814815 0.843713956171q\\\n",
    "400 0.1 0.828985507246 0.842896935933q\\\n",
    "400 0.15 0.82194979568 0.839038189533q\\\n",
    "400 0.2 0.827666376053 0.84127874369q\\\n",
    "400 0.25 0.815245853942 0.840660879306q\\\n",
    "400 0.3 0.823632130384 0.841206602163q\\\n",
    "400 0.35 0.823939570017 0.843946433452q\\\n",
    "400 0.4 0.828943560058 0.842721695482q\\\n",
    "400 0.45 0.832179930796 0.843176038429q\\\n",
    "400 0.5 0.832947976879 0.844910688971q\\\n",
    "450 0.1 0.824006958539 0.840839160839q\\\n",
    "450 0.15 0.826635376943 0.840915512857q\\\n",
    "450 0.2 0.832283915283 0.839896521989q\\\n",
    "450 0.25 0.826441467676 0.840331333905q\\\n",
    "450 0.3 0.82945067587 0.842574533296q\\\n",
    "450 0.35 0.828497558173 0.843120567376q\\\n",
    "450 0.4 0.830503597122 0.840563380282q\\\n",
    "450 0.45 0.826200873362 0.842499280161q\\\n",
    "450 0.5 0.82818365579 0.846590909091q\\\n",
    "500 0.1 0.817971014493 0.837631056957q\\\n",
    "500 0.15 0.820437956204 0.840942134341q\\\n",
    "500 0.2 0.82102189781 0.840876743524q\\\n",
    "500 0.25 0.831700288184 0.842683632157q\\\n",
    "500 0.3 0.822451994092 0.842543737851q\\\n",
    "500 0.35 0.825806451613 0.84225431889q\\\n",
    "500 0.4 0.83044982699 0.844457175594q\\\n",
    "500 0.45 0.826760151914 0.842979942693q\\\n",
    "500 0.5 0.834470989761 0.843353090601q\\\n",
    "550 0.1 0.82428115016 0.839561674628q\\\n",
    "550 0.15 0.820512820513 0.83775481709q\\\n",
    "550 0.2 0.820046893318 0.840312674484q\\\n",
    "550 0.25 0.821491485614 0.839244851259q\\\n",
    "550 0.3 0.816180844735 0.840786100826q\\\n",
    "550 0.35 0.827327760161 0.842017275007q\\\n",
    "550 0.4 0.825125331761 0.844668345928q\\\n",
    "550 0.45 0.829212190914 0.842165568605q\\\n",
    "550 0.5 0.834851936219 0.841206602163q\\\n",
    "600 0.1 0.831977559607 0.838202247191q\\\n",
    "600 0.15 0.829435253851 0.842693489802q\\\n",
    "600 0.2 0.830690826727 0.83867309328q\\\n",
    "600 0.25 0.835770528684 0.839083632349q\\\n",
    "600 0.3 0.827408256881 0.839329854436q\\\n",
    "600 0.35 0.835570469799 0.841309823678q\\\n",
    "600 0.4 0.830804466075 0.841283124128q\\\n",
    "600 0.45 0.830113636364 0.841779279279q\\\n",
    "600 0.5 0.834565093803 0.842282822603q\\\n",
    "650 0.1 0.632674853177 0.632674853177q\\\n",
    "650 0.15 0.632674853177 0.632674853177q\\\n",
    "650 0.2 0.821230679498 0.839160839161q\\\n",
    "650 0.25 0.815943728019 0.841152263374q\\\n",
    "650 0.3 0.824436197545 0.839060710195q\\\n",
    "650 0.35 0.829604130809 0.840236686391q\\\n",
    "650 0.4 0.82711085583 0.841956460277q\\\n",
    "650 0.45 0.831908831909 0.83908045977q\\\n",
    "650 0.5 0.8314415437 0.841057719647q\\\n",
    "700 0.1 0.827076222981 0.837937909428q\\\n",
    "700 0.15 0.828951088493 0.838978345364q\\\n",
    "700 0.2 0.830166954519 0.841501862998q\\\n",
    "700 0.25 0.834672789897 0.842105263158q\\\n",
    "700 0.3 0.826162790698 0.843949930459q\\\n",
    "700 0.35 0.831694296151 0.841126760563q\\\n",
    "700 0.4 0.833996588971 0.840839160839q\\\n",
    "700 0.45 0.829268292683 0.841573033708q\\\n",
    "700 0.5 0.834252539913 0.842970521542\\\n",
    "\"\n",
    "\n",
    "r = []\n",
    "for val in prev.split('q'):\n",
    "  vals = [float(v) for v in val.split(' ')]\n",
    "  r.append(tuple(vals))\n",
    "\n",
    "one_layer_more_844 = sorted(r, key=lambda v: -v[3])[:8]\n",
    "one_layer_more_844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.5 : 0.846590909091\n",
      "350 0.45 : 0.845939933259\n",
      "350 0.3 : 0.845852017937\n",
      "250 0.5 : 0.845094180489\n",
      "400 0.5 : 0.844910688971\n",
      "550 0.4 : 0.844668345928\n",
      "500 0.4 : 0.844457175594\n",
      "350 0.1 : 0.844383561644\n"
     ]
    }
   ],
   "source": [
    "for t in one_layer_more_844:\n",
    "  neu, dro = int(t[0]), np.float64(t[1])\n",
    "  print neu, t[1], ':',\n",
    "  \n",
    "  model = get_1_layer_model(neu, dro)\n",
    "  model.fit(Xtrain, ytrain, verbose=0, callbacks=[f1stopperCallback()], nb_epoch=200)\n",
    "  model.load_weights('models/tmp_weights.h5')\n",
    "    \n",
    "  pred = model.predict(Xval)\n",
    "  pred[pred < .5] = 0\n",
    "  pred[pred >= .5] = 1\n",
    "  f1_val_best = f1_score(yval, pred, average='micro')\n",
    "\n",
    "  print f1_val_best\n",
    "\n",
    "  models.append((f1_val_best, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_2_layer_model(n_neurons, dropout_val):\n",
    "  model = Sequential()\n",
    "  np.random.seed(0)\n",
    "  \n",
    "  model.add(Dense(n_neurons, input_shape=(4096,), activation='softplus'))\n",
    "  model.add(Dropout(dropout_val))\n",
    "  \n",
    "  model.add(Dense(n_neurons, activation='softplus'))\n",
    "  model.add(Dropout(dropout_val))\n",
    "\n",
    "  model.add(Dense(9, activation='sigmoid'))\n",
    "  model.compile(loss=loss, optimizer='adam')\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.1 0.826200873362 0.845039018952\n",
      "50 0.15 0.822575976845 0.840883352208\n",
      "50 0.2 0.82562902282 0.841037204059\n",
      "50 0.25 0.828066106118 0.839630562552\n",
      "50 0.3 0.837666088966 0.84630404463\n",
      "50 0.35 0.835957629545 0.840283687943\n",
      "50 0.4 0.827586206897 0.840902758429\n",
      "50 0.45 0.828660436137 0.838095238095\n",
      "50 0.5 0.828579497317 0.835225699252\n",
      "100 0.1 0.817785527463 0.840839160839\n",
      "100 0.15 0.821089023336 0.840871021776\n",
      "100 0.2 0.82318501171 0.84284509418\n",
      "100 0.25 0.826279964487 0.842075578116\n",
      "100 0.3 0.827984966753 0.843264614516\n",
      "100 0.35 0.836322231711 0.842792281498\n",
      "100 0.4 0.835152211373 0.841985168283\n",
      "100 0.45 0.838987924094 0.842163970998\n",
      "100 0.5 0.839782172542 0.839782172542\n",
      "150 0.1 0.830177854274 0.845982142857\n",
      "150 0.15 0.829000577701 0.844895657078\n",
      "150 0.2 0.825248392753 0.84440706476\n",
      "150 0.25 0.827868852459 0.842371452655\n",
      "150 0.3 0.834907010014 0.844837421563\n",
      "150 0.35 0.838985465945 0.840687679083\n",
      "150 0.4 0.839750849377 0.842016333427\n",
      "150 0.45 0.840909090909 0.844406587166\n",
      "150 0.5 0.840660879306 0.842458100559\n",
      "200 0.1 0.820886814469 0.844884488449\n",
      "200 0.15 0.823259810942 0.841694537347\n",
      "200 0.2 0.827149841818 0.843408175014\n",
      "200 0.25 0.823357664234 0.842312008979\n",
      "200 0.3 0.830795748348 0.843741169822\n",
      "200 0.35 0.834968263128 0.841896111269\n",
      "200 0.4 0.83284457478 0.844581349762\n",
      "200 0.45 0.838299103269 0.844646407271\n",
      "200 0.5 0.838001732602 0.844998583168\n",
      "250 0.1 0.828248587571 0.840832395951\n",
      "250 0.15 0.831913685406 0.841756420878\n",
      "250 0.2 0.828870779977 0.84171848502\n",
      "250 0.25 0.822873900293 0.842075892857\n",
      "250 0.3 0.831034482759 0.844031531532\n",
      "250 0.35 0.829113924051 0.841116173121\n",
      "250 0.4 0.831837916064 0.846305141894\n",
      "250 0.45 0.823766945486 0.844798180785\n",
      "250 0.5 0.829128440367 0.841224018476\n",
      "300 0.1 0.820795817601 0.839831697055\n",
      "300 0.15 0.817285382831 0.840233398166\n",
      "300 0.2 0.832245245529 0.842075256556\n",
      "300 0.25 0.821347031963 0.83904109589\n",
      "300 0.3 0.821366024518 0.848384424192\n",
      "300 0.35 0.823149236193 0.841865756542\n",
      "300 0.4 0.836279339322 0.843891402715\n",
      "300 0.45 0.831797235023 0.842165568605\n",
      "300 0.5 0.832095945174 0.843490701001\n",
      "350 0.1 0.82612533098 0.84154235857\n",
      "350 0.15 0.827293318233 0.837362637363\n",
      "350 0.2 0.827389443652 0.83978824185\n",
      "350 0.25 0.823462088698 0.842395804582\n",
      "350 0.3 0.823899371069 0.841367713004\n",
      "350 0.35 0.82264586944 0.841359773371\n",
      "350 0.4 0.827546296296 0.84606613455\n",
      "350 0.45 0.827020915772 0.845319634703\n",
      "350 0.5 0.831042382589 0.844078386822\n",
      "400 0.1 0.823462414579 0.841896111269\n",
      "400 0.15 0.826928633343 0.841724617524\n",
      "400 0.2 0.824964539007 0.841539332201\n",
      "400 0.25 0.824849007765 0.843137254902\n",
      "400 0.3 0.830489192264 0.844943820225\n",
      "400 0.35 0.827705627706 0.843533647189\n",
      "400 0.4 0.831486217675 0.842965886665\n",
      "400 0.45 0.831949613513 0.844166903207\n",
      "400 0.5 0.831047563467 0.844119302195\n",
      "450 0.1 0.827684420393 0.838526912181\n",
      "450 0.15 0.828003457217 0.839321644151\n",
      "450 0.2 0.829142857143 0.840695354802\n",
      "450 0.25 0.820292347377 0.839554317549\n",
      "450 0.3 0.825757575758 0.842015371477\n",
      "450 0.35 0.824374820351 0.841224018476\n",
      "450 0.4 0.827966347549 0.843634292224\n",
      "450 0.45 0.832664756447 0.841305571187\n",
      "450 0.5 0.844137136272 0.846022727273\n",
      "500 0.1 0.822632659044 0.840889890172\n",
      "500 0.15 0.834090909091 0.83995584989\n",
      "500 0.2 0.828341013825 0.843246247916\n",
      "500 0.25 0.833944178179 0.84524137931\n",
      "500 0.3 0.818468597648 0.840979453983\n",
      "500 0.35 0.826939471441 0.84180949705\n",
      "500 0.4 0.830167280975 0.842703009654\n",
      "500 0.45 0.829379822807 0.843837144451\n",
      "500 0.5 0.834233721922 0.843359818388\n",
      "550 0.1 0.82481962482 0.837563451777\n",
      "550 0.15 0.832087406555 0.839021615472\n",
      "550 0.2 0.824934459656 0.841724617524\n",
      "550 0.25 0.827485380117 0.841340782123\n",
      "550 0.3 0.823322590097 0.843219159009\n",
      "550 0.35"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 9011200 bytes of device memory (out of memory).\nApply node that caused the error: GpuDot22Scalar(GpuDimShuffle{1,0}.0, GpuElemwise{Composite{((i0 * i1) * scalar_sigmoid(i2))}}[(0, 0)].0, HostFromGpu.0)\nToposort index: 112\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), TensorType(float32, scalar)]\nInputs shapes: [(4096, 128), (128, 550), ()]\nInputs strides: [(1, 4096), (550, 1), ()]\nInputs values: ['not shown', 'not shown', array(0.10000002384185791, dtype=float32)]\nOutputs clients: [[GpuElemwise{Composite{((i0 * i1) + i2)}}[(0, 1)](GpuDimShuffle{x,x}.0, <CudaNdarrayType(float32, matrix)>, GpuDot22Scalar.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-26c1b49e6c61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_2_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf1stopperCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, show_accuracy, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    699\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                          \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m                          shuffle=shuffle, metrics=metrics)\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, metrics)\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Error allocating 9011200 bytes of device memory (out of memory).\nApply node that caused the error: GpuDot22Scalar(GpuDimShuffle{1,0}.0, GpuElemwise{Composite{((i0 * i1) * scalar_sigmoid(i2))}}[(0, 0)].0, HostFromGpu.0)\nToposort index: 112\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), TensorType(float32, scalar)]\nInputs shapes: [(4096, 128), (128, 550), ()]\nInputs strides: [(1, 4096), (550, 1), ()]\nInputs values: ['not shown', 'not shown', array(0.10000002384185791, dtype=float32)]\nOutputs clients: [[GpuElemwise{Composite{((i0 * i1) + i2)}}[(0, 1)](GpuDimShuffle{x,x}.0, <CudaNdarrayType(float32, matrix)>, GpuDot22Scalar.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "for neu in neurons:\n",
    "  for dro in dropout_vals:\n",
    "    print neu, dro,\n",
    "    model = get_2_layer_model(neu, dro)\n",
    "    \n",
    "    model.fit(Xtrain, ytrain, verbose=0, callbacks=[f1stopperCallback()], nb_epoch=200)\n",
    "    \n",
    "    pred = model.predict(Xval)\n",
    "    pred[pred < .5] = 0\n",
    "    pred[pred >= .5] = 1\n",
    "    f1_val_last = f1_score(yval, pred, average='micro')\n",
    "    \n",
    "    model.load_weights('models/tmp_weights.h5')\n",
    "    \n",
    "    pred = model.predict(Xval)\n",
    "    pred[pred < .5] = 0\n",
    "    pred[pred >= .5] = 1\n",
    "    f1_val_best = f1_score(yval, pred, average='micro')\n",
    "    \n",
    "    print f1_val_last, f1_val_best\n",
    "    \n",
    "    models.append((f1_val_best, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(300.0, 0.3, 0.821366024518, 0.848384424192),\n",
       " (600.0, 0.35, 0.833285998296, 0.846543001686),\n",
       " (250.0, 0.4, 0.831837916064, 0.846305141894),\n",
       " (50.0, 0.3, 0.837666088966, 0.84630404463),\n",
       " (350.0, 0.4, 0.827546296296, 0.84606613455),\n",
       " (450.0, 0.5, 0.844137136272, 0.846022727273),\n",
       " (150.0, 0.1, 0.830177854274, 0.845982142857),\n",
       " (550.0, 0.45, 0.830318690784, 0.845464725644),\n",
       " (650.0, 0.3, 0.835734870317, 0.845390070922),\n",
       " (350.0, 0.45, 0.827020915772, 0.845319634703),\n",
       " (500.0, 0.25, 0.833944178179, 0.84524137931),\n",
       " (50.0, 0.1, 0.826200873362, 0.845039018952),\n",
       " (200.0, 0.5, 0.838001732602, 0.844998583168),\n",
       " (400.0, 0.3, 0.830489192264, 0.844943820225),\n",
       " (150.0, 0.15, 0.829000577701, 0.844895657078),\n",
       " (200.0, 0.1, 0.820886814469, 0.844884488449),\n",
       " (150.0, 0.3, 0.834907010014, 0.844837421563),\n",
       " (250.0, 0.45, 0.823766945486, 0.844798180785),\n",
       " (200.0, 0.45, 0.838299103269, 0.844646407271),\n",
       " (200.0, 0.4, 0.83284457478, 0.844581349762),\n",
       " (150.0, 0.2, 0.825248392753, 0.84440706476),\n",
       " (150.0, 0.45, 0.840909090909, 0.844406587166),\n",
       " (400.0, 0.45, 0.831949613513, 0.844166903207),\n",
       " (400.0, 0.5, 0.831047563467, 0.844119302195),\n",
       " (350.0, 0.5, 0.831042382589, 0.844078386822),\n",
       " (250.0, 0.3, 0.831034482759, 0.844031531532)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev = \"\\\n",
    "50 0.1 0.826200873362 0.845039018952q\\\n",
    "50 0.15 0.822575976845 0.840883352208q\\\n",
    "50 0.2 0.82562902282 0.841037204059q\\\n",
    "50 0.25 0.828066106118 0.839630562552q\\\n",
    "50 0.3 0.837666088966 0.84630404463q\\\n",
    "50 0.35 0.835957629545 0.840283687943q\\\n",
    "50 0.4 0.827586206897 0.840902758429q\\\n",
    "50 0.45 0.828660436137 0.838095238095q\\\n",
    "50 0.5 0.828579497317 0.835225699252q\\\n",
    "100 0.1 0.817785527463 0.840839160839q\\\n",
    "100 0.15 0.821089023336 0.840871021776q\\\n",
    "100 0.2 0.82318501171 0.84284509418q\\\n",
    "100 0.25 0.826279964487 0.842075578116q\\\n",
    "100 0.3 0.827984966753 0.843264614516q\\\n",
    "100 0.35 0.836322231711 0.842792281498q\\\n",
    "100 0.4 0.835152211373 0.841985168283q\\\n",
    "100 0.45 0.838987924094 0.842163970998q\\\n",
    "100 0.5 0.839782172542 0.839782172542q\\\n",
    "150 0.1 0.830177854274 0.845982142857q\\\n",
    "150 0.15 0.829000577701 0.844895657078q\\\n",
    "150 0.2 0.825248392753 0.84440706476q\\\n",
    "150 0.25 0.827868852459 0.842371452655q\\\n",
    "150 0.3 0.834907010014 0.844837421563q\\\n",
    "150 0.35 0.838985465945 0.840687679083q\\\n",
    "150 0.4 0.839750849377 0.842016333427q\\\n",
    "150 0.45 0.840909090909 0.844406587166q\\\n",
    "150 0.5 0.840660879306 0.842458100559q\\\n",
    "200 0.1 0.820886814469 0.844884488449q\\\n",
    "200 0.15 0.823259810942 0.841694537347q\\\n",
    "200 0.2 0.827149841818 0.843408175014q\\\n",
    "200 0.25 0.823357664234 0.842312008979q\\\n",
    "200 0.3 0.830795748348 0.843741169822q\\\n",
    "200 0.35 0.834968263128 0.841896111269q\\\n",
    "200 0.4 0.83284457478 0.844581349762q\\\n",
    "200 0.45 0.838299103269 0.844646407271q\\\n",
    "200 0.5 0.838001732602 0.844998583168q\\\n",
    "250 0.1 0.828248587571 0.840832395951q\\\n",
    "250 0.15 0.831913685406 0.841756420878q\\\n",
    "250 0.2 0.828870779977 0.84171848502q\\\n",
    "250 0.25 0.822873900293 0.842075892857q\\\n",
    "250 0.3 0.831034482759 0.844031531532q\\\n",
    "250 0.35 0.829113924051 0.841116173121q\\\n",
    "250 0.4 0.831837916064 0.846305141894q\\\n",
    "250 0.45 0.823766945486 0.844798180785q\\\n",
    "250 0.5 0.829128440367 0.841224018476q\\\n",
    "300 0.1 0.820795817601 0.839831697055q\\\n",
    "300 0.15 0.817285382831 0.840233398166q\\\n",
    "300 0.2 0.832245245529 0.842075256556q\\\n",
    "300 0.25 0.821347031963 0.83904109589q\\\n",
    "300 0.3 0.821366024518 0.848384424192q\\\n",
    "300 0.35 0.823149236193 0.841865756542q\\\n",
    "300 0.4 0.836279339322 0.843891402715q\\\n",
    "300 0.45 0.831797235023 0.842165568605q\\\n",
    "300 0.5 0.832095945174 0.843490701001q\\\n",
    "350 0.1 0.82612533098 0.84154235857q\\\n",
    "350 0.15 0.827293318233 0.837362637363q\\\n",
    "350 0.2 0.827389443652 0.83978824185q\\\n",
    "350 0.25 0.823462088698 0.842395804582q\\\n",
    "350 0.3 0.823899371069 0.841367713004q\\\n",
    "350 0.35 0.82264586944 0.841359773371q\\\n",
    "350 0.4 0.827546296296 0.84606613455q\\\n",
    "350 0.45 0.827020915772 0.845319634703q\\\n",
    "350 0.5 0.831042382589 0.844078386822q\\\n",
    "400 0.1 0.823462414579 0.841896111269q\\\n",
    "400 0.15 0.826928633343 0.841724617524q\\\n",
    "400 0.2 0.824964539007 0.841539332201q\\\n",
    "400 0.25 0.824849007765 0.843137254902q\\\n",
    "400 0.3 0.830489192264 0.844943820225q\\\n",
    "400 0.35 0.827705627706 0.843533647189q\\\n",
    "400 0.4 0.831486217675 0.842965886665q\\\n",
    "400 0.45 0.831949613513 0.844166903207q\\\n",
    "400 0.5 0.831047563467 0.844119302195q\\\n",
    "450 0.1 0.827684420393 0.838526912181q\\\n",
    "450 0.15 0.828003457217 0.839321644151q\\\n",
    "450 0.2 0.829142857143 0.840695354802q\\\n",
    "450 0.25 0.820292347377 0.839554317549q\\\n",
    "450 0.3 0.825757575758 0.842015371477q\\\n",
    "450 0.35 0.824374820351 0.841224018476q\\\n",
    "450 0.4 0.827966347549 0.843634292224q\\\n",
    "450 0.45 0.832664756447 0.841305571187q\\\n",
    "450 0.5 0.844137136272 0.846022727273q\\\n",
    "500 0.1 0.822632659044 0.840889890172q\\\n",
    "500 0.15 0.834090909091 0.83995584989q\\\n",
    "500 0.2 0.828341013825 0.843246247916q\\\n",
    "500 0.25 0.833944178179 0.84524137931q\\\n",
    "500 0.3 0.818468597648 0.840979453983q\\\n",
    "500 0.35 0.826939471441 0.84180949705q\\\n",
    "500 0.4 0.830167280975 0.842703009654q\\\n",
    "500 0.45 0.829379822807 0.843837144451q\\\n",
    "500 0.5 0.834233721922 0.843359818388q\\\n",
    "550 0.1 0.82481962482 0.837563451777q\\\n",
    "550 0.15 0.832087406555 0.839021615472q\\\n",
    "550 0.2 0.824934459656 0.841724617524q\\\n",
    "550 0.25 0.827485380117 0.841340782123q\\\n",
    "550 0.3 0.823322590097 0.843219159009q\\\n",
    "550 0.35 0.830513335245 0.843615494978q\\\n",
    "550 0.4 0.830725462304 0.841545352744q\\\n",
    "550 0.45 0.830318690784 0.845464725644q\\\n",
    "550 0.5 0.836805555556 0.842105263158q\\\n",
    "600 0.1 0.821602987647 0.838346921563q\\\n",
    "600 0.15 0.833656241351 0.83972027972q\\\n",
    "600 0.2 0.824956672444 0.842105263158q\\\n",
    "600 0.25 0.825442099258 0.840832395951q\\\n",
    "600 0.3 0.826674331705 0.840226628895q\\\n",
    "600 0.35 0.833285998296 0.846543001686q\\\n",
    "600 0.4 0.826163818503 0.843454038997q\\\n",
    "600 0.45 0.826174690112 0.843907916901q\\\n",
    "600 0.5 0.824682814302 0.842781345993q\\\n",
    "650 0.1 0.828751431844 0.836664762993q\\\n",
    "650 0.15 0.826436781609 0.841558441558q\\\n",
    "650 0.2 0.82702858793 0.840742824986q\\\n",
    "650 0.25 0.827766482719 0.842671194114q\\\n",
    "650 0.3 0.835734870317 0.845390070922q\\\n",
    "650 0.35 0.824153166421 0.840979453983q\\\n",
    "650 0.4 0.836630864895 0.840876743524q\\\n",
    "650 0.45 0.830155470813 0.843441466855q\\\n",
    "650 0.5 0.832662449684 0.841893830703q\\\n",
    "700 0.1 0.525458248473 0.525458248473q\\\n",
    "700 0.15 0.636412172985 0.636412172985q\\\n",
    "700 0.2 0.827843024921 0.83994374121q\\\n",
    "700 0.25 0.82585596222 0.840767927724q\\\n",
    "700 0.3 0.826705370102 0.839807201588q\\\n",
    "700 0.35 0.826099621322 0.841206602163q\\\n",
    "700 0.4 0.824351878823 0.842642472356q\\\n",
    "700 0.45 0.829971181556 0.841059602649q\\\n",
    "700 0.5 0.837410071942 0.843064653945\\\n",
    "\"\n",
    "\n",
    "r = []\n",
    "for val in prev.split('q'):\n",
    "  vals = [float(v) for v in val.split(' ')]\n",
    "  r.append(tuple(vals))\n",
    "sorted(r, key=lambda v: -v[3])[:26]\n",
    "\n",
    "two_layer_more_844 = sorted(r, key=lambda v: -v[3])[:26]\n",
    "two_layer_more_844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.3 : 0.848384424192\n",
      "600 0.35 : 0.846543001686\n",
      "250 0.4 : 0.846305141894\n",
      "50 0.3 : 0.84630404463\n",
      "350 0.4 : 0.84606613455\n",
      "450 0.5 : 0.846022727273\n",
      "150 0.1 : 0.845982142857\n",
      "550 0.45 : 0.845464725644\n",
      "650 0.3 : 0.845390070922\n",
      "350 0.45 : 0.845319634703\n",
      "500 0.25 : 0.84524137931\n",
      "50 0.1 : 0.845039018952\n",
      "200 0.5 : 0.844998583168\n",
      "400 0.3 : 0.844943820225\n",
      "150 0.15 : 0.844895657078\n",
      "200 0.1 : 0.844884488449\n",
      "150 0.3 : 0.844837421563\n",
      "250 0.45 : 0.844798180785\n",
      "200 0.45 : 0.844646407271\n",
      "200 0.4 : 0.844581349762\n",
      "150 0.2 : 0.84440706476\n",
      "150 0.45 : 0.844406587166\n",
      "400 0.45 : 0.844166903207\n",
      "400 0.5 : 0.844119302195\n",
      "350 0.5 : 0.844078386822\n",
      "250 0.3 : 0.844031531532\n"
     ]
    }
   ],
   "source": [
    "for t in two_layer_more_844:\n",
    "  neu, dro = int(t[0]), np.float64(t[1])\n",
    "  print neu, t[1], ':',\n",
    "  \n",
    "  model = get_2_layer_model(neu, dro)\n",
    "  model.fit(Xtrain, ytrain, verbose=0, callbacks=[f1stopperCallback()], nb_epoch=200)\n",
    "  model.load_weights('models/tmp_weights.h5')\n",
    "    \n",
    "  pred = model.predict(Xval)\n",
    "  pred[pred < .5] = 0\n",
    "  pred[pred >= .5] = 1\n",
    "  f1_val_best = f1_score(yval, pred, average='micro')\n",
    "\n",
    "  print f1_val_best\n",
    "\n",
    "  models.append((f1_val_best, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-layer nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_3_layer_model(n_neurons, dropout_val):\n",
    "  model = Sequential()\n",
    "  np.random.seed(0)\n",
    "  \n",
    "  model.add(Dense(n_neurons, input_shape=(4096,), activation='softplus'))\n",
    "  model.add(Dropout(dropout_val))\n",
    "  \n",
    "  model.add(Dense(n_neurons, activation='softplus'))\n",
    "  model.add(Dropout(dropout_val))\n",
    "  \n",
    "  model.add(Dense(n_neurons, activation='softplus'))\n",
    "  model.add(Dropout(dropout_val))\n",
    "\n",
    "  model.add(Dense(9, activation='sigmoid'))\n",
    "  model.compile(loss=loss, optimizer='adam')\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.1 0.829625457618 0.842224103867\n",
      "50 0.15 0.83063583815 0.844031531532\n",
      "50 0.2 0.833476149672 0.842164599775\n",
      "50 0.25 0.827306913509 0.844571428571\n",
      "50 0.3 0.832027850305 0.840490797546\n",
      "50 0.35 0.823152995928 0.831161473088\n",
      "50 0.4 0.824200913242 0.831977559607\n",
      "50 0.45 0.824506155167 0.830369743156\n",
      "50 0.5 0.813230681494 0.818953711312\n",
      "100 0.1 0.818420285631 0.841657207719\n",
      "100 0.15 0.823633156966 0.842792281498\n",
      "100 0.2 0.828377980571 0.843319268636\n",
      "100 0.25 0.830308268511 0.84329199549\n",
      "100 0.3 0.821071218364 0.841839596186\n",
      "100 0.35 0.835120254999 0.841956460277\n",
      "100 0.4 0.8265129683 0.84009009009\n",
      "100 0.45 0.829924348557 0.83994374121\n",
      "100 0.5 0.828808230923 0.834324553951\n",
      "150 0.1 0.828767123288 0.840695354802\n",
      "150 0.15 0.834797393029 0.841901603096\n",
      "150 0.2 0.829873125721 0.841463414634\n",
      "150 0.25 0.83003725996 0.842665173572\n",
      "150 0.3 0.826036866359 0.842105263158\n",
      "150 0.35 0.838525292941 0.842256764537\n",
      "150 0.4 0.828719723183 0.845431255337\n",
      "150 0.45 0.835507657402 0.837822671156\n",
      "150 0.5 0.828854314003 0.8342760181\n",
      "200 0.1 0.82754727632 0.842193153354\n",
      "200 0.15 0.830934236523 0.846543001686\n",
      "200 0.2 0.825072886297 0.845787853345\n",
      "200 0.25 0.83338080319 0.840818646958\n",
      "200 0.3 0.835471913316 0.84512645638\n",
      "200 0.35 0.833766984678 0.845261669024\n",
      "200 0.4 0.831454177508 0.846622369878\n",
      "200 0.45 0.836248924577 0.845086377797\n",
      "200 0.5 0.832472748135 0.84161849711\n",
      "250 0.1 0.825497548313 0.843360133075\n",
      "250 0.15 0.828886310905 0.847065462754\n",
      "250 0.2 0.825693430657 0.843115124153\n",
      "250 0.25 0.829368575624 0.842438182864\n",
      "250 0.3 0.831102250071 0.843837144451\n",
      "250 0.35 0.837640449438 0.843041606887\n",
      "250 0.4 0.8352402746 0.844507042254\n",
      "250 0.45 0.840022611645 0.842639593909\n",
      "250 0.5 0.838089855891 0.841449603624\n",
      "300 0.1 0.813114754098 0.842721695482\n",
      "300 0.15 0.828637147786 0.844280860702\n",
      "300 0.2 0.83010130246 0.844444444444\n",
      "300 0.25 0.828637147786 0.841463414634\n",
      "300 0.3 0.834030683403 0.841078600115\n",
      "300 0.35 0.833804409271 0.84280565567\n",
      "300 0.4 0.842165242165 0.843421426542\n",
      "300 0.45 0.836353340883 0.845313398505\n",
      "300 0.5 0.839633447881 0.841074426413\n",
      "350 0.1 0.824169530355 0.84284509418\n",
      "350 0.15 0.824039295001 0.841598186455\n",
      "350 0.2 0.820558526441 0.843917583968\n",
      "350 0.25 0.82684105772 0.843583263128\n",
      "350 0.3 0.818580192813 0.842841163311\n",
      "350 0.35 0.830698476574 0.843148357871\n",
      "350 0.4 0.828776978417 0.845272206304\n",
      "350 0.45 0.83419988446 0.845936794582\n",
      "350 0.5 0.839965645577 0.844266970907\n",
      "400 0.1 0.817770232032 0.838963963964\n",
      "400 0.15 0.822402785839 0.840068298236\n",
      "400 0.2 0.829144309517 0.84107987754\n",
      "400 0.25 0.816486161252 0.841985815603\n",
      "400 0.3 0.833188782885 0.841775754126\n",
      "400 0.35 0.827259259259 0.841453111799\n",
      "400 0.4 0.829000577701 0.843541202673\n",
      "400 0.45 0.835984958056 0.84406779661\n",
      "400 0.5 0.835667333524 0.84406779661\n",
      "450 0.1 0.821971830986 0.841606246514\n",
      "450 0.15 0.830362277171 0.84018768976\n",
      "450 0.2 0.815305815005 0.840407470289\n",
      "450 0.25 0.818751866229 0.840973401245\n",
      "450 0.3 0.826036193812 0.840909090909\n",
      "450 0.35 0.836209335219 0.842763719079\n",
      "450 0.4 0.832552693208 0.843199537973\n",
      "450 0.45 0.835051546392 0.846912884127\n",
      "450 0.5 0.840695354802 0.844005641749\n",
      "500 0.1 0.826818830243 0.840413943355\n",
      "500 0.15 0.831778343229 0.844594594595\n",
      "500 0.2 0.833996588971 0.842490842491\n",
      "500 0.25 0.827080394922 0.844720496894\n",
      "500 0.3 0.825239895318 0.846240179574\n",
      "500 0.35 0.834807417974 0.841536273115\n",
      "500 0.4 0.833765485451 0.845046570703\n",
      "500 0.45 0.832904884319 0.843359818388\n",
      "500 0.5 0.828855140187 0.843714609287\n",
      "550 0.1 0.827367205543 0.839187705818\n",
      "550 0.15"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "('Error allocating 9011200 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cf15af7aadae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mdro\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdropout_vals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mneu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdro\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_3_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf1stopperCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-07ea6417e7ea>\u001b[0m in \u001b[0;36mget_3_layer_model\u001b[1;34m(n_neurons, dropout_val)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, class_mode, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m         updates = self.optimizer.get_updates(self.trainable_weights,\n\u001b[0;32m    549\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m                                              train_loss)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[0mupdates\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/optimizers.pyc\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, params, constraints, loss)\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;31m# zero init of velocity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[0mm_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mvariable\u001b[1;34m(value, dtype, name)\u001b[0m\n\u001b[0;32m     34\u001b[0m     '''\n\u001b[0;32m     35\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/theano/compile/sharedvalue.pyc\u001b[0m in \u001b[0;36mshared\u001b[1;34m(value, name, strict, allow_downcast, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                 var = ctor(value, name=name, strict=strict,\n\u001b[1;32m--> 247\u001b[1;33m                            allow_downcast=allow_downcast, **kwargs)\n\u001b[0m\u001b[0;32m    248\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/budmitr/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/var.pyc\u001b[0m in \u001b[0;36mfloat32_shared_constructor\u001b[1;34m(value, name, strict, allow_downcast, borrow, broadcastable, target)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;31m# type.broadcastable is guaranteed to be a tuple, which this next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# function requires\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mdeviceval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_support_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcastable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: ('Error allocating 9011200 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")"
     ]
    }
   ],
   "source": [
    "for neu in neurons:\n",
    "  for dro in dropout_vals:\n",
    "    print neu, dro,\n",
    "    model = get_3_layer_model(neu, dro)\n",
    "    \n",
    "    model.fit(Xtrain, ytrain, verbose=0, callbacks=[f1stopperCallback()], nb_epoch=200)\n",
    "    \n",
    "    pred = model.predict(Xval)\n",
    "    pred[pred < .5] = 0\n",
    "    pred[pred >= .5] = 1\n",
    "    f1_val_last = f1_score(yval, pred, average='micro')\n",
    "    \n",
    "    model.load_weights('models/tmp_weights.h5')\n",
    "    \n",
    "    pred = model.predict(Xval)\n",
    "    pred[pred < .5] = 0\n",
    "    pred[pred >= .5] = 1\n",
    "    f1_val_best = f1_score(yval, pred, average='micro')\n",
    "    \n",
    "    print f1_val_last, f1_val_best\n",
    "    \n",
    "    models.append((f1_val_best, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(600.0, 0.5, 0.83013379872, 0.848054529963),\n",
       " (250.0, 0.15, 0.828886310905, 0.847065462754),\n",
       " (450.0, 0.45, 0.835051546392, 0.846912884127),\n",
       " (200.0, 0.4, 0.831454177508, 0.846622369878),\n",
       " (200.0, 0.15, 0.830934236523, 0.846543001686),\n",
       " (500.0, 0.3, 0.825239895318, 0.846240179574),\n",
       " (650.0, 0.45, 0.835734870317, 0.846000569314),\n",
       " (350.0, 0.45, 0.83419988446, 0.845936794582),\n",
       " (200.0, 0.2, 0.825072886297, 0.845787853345),\n",
       " (150.0, 0.4, 0.828719723183, 0.845431255337),\n",
       " (300.0, 0.45, 0.836353340883, 0.845313398505),\n",
       " (350.0, 0.4, 0.828776978417, 0.845272206304),\n",
       " (200.0, 0.35, 0.833766984678, 0.845261669024),\n",
       " (200.0, 0.3, 0.835471913316, 0.84512645638),\n",
       " (200.0, 0.45, 0.836248924577, 0.845086377797),\n",
       " (500.0, 0.4, 0.833765485451, 0.845046570703),\n",
       " (550.0, 0.5, 0.836646499568, 0.844808126411),\n",
       " (500.0, 0.25, 0.827080394922, 0.844720496894),\n",
       " (500.0, 0.15, 0.831778343229, 0.844594594595),\n",
       " (50.0, 0.25, 0.827306913509, 0.844571428571),\n",
       " (250.0, 0.4, 0.8352402746, 0.844507042254),\n",
       " (300.0, 0.2, 0.83010130246, 0.844444444444),\n",
       " (300.0, 0.15, 0.828637147786, 0.844280860702),\n",
       " (350.0, 0.5, 0.839965645577, 0.844266970907),\n",
       " (400.0, 0.45, 0.835984958056, 0.84406779661),\n",
       " (400.0, 0.5, 0.835667333524, 0.84406779661),\n",
       " (50.0, 0.15, 0.83063583815, 0.844031531532),\n",
       " (450.0, 0.5, 0.840695354802, 0.844005641749)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev = \"\\\n",
    "50 0.1 0.829625457618 0.842224103867q\\\n",
    "50 0.15 0.83063583815 0.844031531532q\\\n",
    "50 0.2 0.833476149672 0.842164599775q\\\n",
    "50 0.25 0.827306913509 0.844571428571q\\\n",
    "50 0.3 0.832027850305 0.840490797546q\\\n",
    "50 0.35 0.823152995928 0.831161473088q\\\n",
    "50 0.4 0.824200913242 0.831977559607q\\\n",
    "50 0.45 0.824506155167 0.830369743156q\\\n",
    "50 0.5 0.813230681494 0.818953711312q\\\n",
    "100 0.1 0.818420285631 0.841657207719q\\\n",
    "100 0.15 0.823633156966 0.842792281498q\\\n",
    "100 0.2 0.828377980571 0.843319268636q\\\n",
    "100 0.25 0.830308268511 0.84329199549q\\\n",
    "100 0.3 0.821071218364 0.841839596186q\\\n",
    "100 0.35 0.835120254999 0.841956460277q\\\n",
    "100 0.4 0.8265129683 0.84009009009q\\\n",
    "100 0.45 0.829924348557 0.83994374121q\\\n",
    "100 0.5 0.828808230923 0.834324553951q\\\n",
    "150 0.1 0.828767123288 0.840695354802q\\\n",
    "150 0.15 0.834797393029 0.841901603096q\\\n",
    "150 0.2 0.829873125721 0.841463414634q\\\n",
    "150 0.25 0.83003725996 0.842665173572q\\\n",
    "150 0.3 0.826036866359 0.842105263158q\\\n",
    "150 0.35 0.838525292941 0.842256764537q\\\n",
    "150 0.4 0.828719723183 0.845431255337q\\\n",
    "150 0.45 0.835507657402 0.837822671156q\\\n",
    "150 0.5 0.828854314003 0.8342760181q\\\n",
    "200 0.1 0.82754727632 0.842193153354q\\\n",
    "200 0.15 0.830934236523 0.846543001686q\\\n",
    "200 0.2 0.825072886297 0.845787853345q\\\n",
    "200 0.25 0.83338080319 0.840818646958q\\\n",
    "200 0.3 0.835471913316 0.84512645638q\\\n",
    "200 0.35 0.833766984678 0.845261669024q\\\n",
    "200 0.4 0.831454177508 0.846622369878q\\\n",
    "200 0.45 0.836248924577 0.845086377797q\\\n",
    "200 0.5 0.832472748135 0.84161849711q\\\n",
    "250 0.1 0.825497548313 0.843360133075q\\\n",
    "250 0.15 0.828886310905 0.847065462754q\\\n",
    "250 0.2 0.825693430657 0.843115124153q\\\n",
    "250 0.25 0.829368575624 0.842438182864q\\\n",
    "250 0.3 0.831102250071 0.843837144451q\\\n",
    "250 0.35 0.837640449438 0.843041606887q\\\n",
    "250 0.4 0.8352402746 0.844507042254q\\\n",
    "250 0.45 0.840022611645 0.842639593909q\\\n",
    "250 0.5 0.838089855891 0.841449603624q\\\n",
    "300 0.1 0.813114754098 0.842721695482q\\\n",
    "300 0.15 0.828637147786 0.844280860702q\\\n",
    "300 0.2 0.83010130246 0.844444444444q\\\n",
    "300 0.25 0.828637147786 0.841463414634q\\\n",
    "300 0.3 0.834030683403 0.841078600115q\\\n",
    "300 0.35 0.833804409271 0.84280565567q\\\n",
    "300 0.4 0.842165242165 0.843421426542q\\\n",
    "300 0.45 0.836353340883 0.845313398505q\\\n",
    "300 0.5 0.839633447881 0.841074426413q\\\n",
    "350 0.1 0.824169530355 0.84284509418q\\\n",
    "350 0.15 0.824039295001 0.841598186455q\\\n",
    "350 0.2 0.820558526441 0.843917583968q\\\n",
    "350 0.25 0.82684105772 0.843583263128q\\\n",
    "350 0.3 0.818580192813 0.842841163311q\\\n",
    "350 0.35 0.830698476574 0.843148357871q\\\n",
    "350 0.4 0.828776978417 0.845272206304q\\\n",
    "350 0.45 0.83419988446 0.845936794582q\\\n",
    "350 0.5 0.839965645577 0.844266970907q\\\n",
    "400 0.1 0.817770232032 0.838963963964q\\\n",
    "400 0.15 0.822402785839 0.840068298236q\\\n",
    "400 0.2 0.829144309517 0.84107987754q\\\n",
    "400 0.25 0.816486161252 0.841985815603q\\\n",
    "400 0.3 0.833188782885 0.841775754126q\\\n",
    "400 0.35 0.827259259259 0.841453111799q\\\n",
    "400 0.4 0.829000577701 0.843541202673q\\\n",
    "400 0.45 0.835984958056 0.84406779661q\\\n",
    "400 0.5 0.835667333524 0.84406779661q\\\n",
    "450 0.1 0.821971830986 0.841606246514q\\\n",
    "450 0.15 0.830362277171 0.84018768976q\\\n",
    "450 0.2 0.815305815005 0.840407470289q\\\n",
    "450 0.25 0.818751866229 0.840973401245q\\\n",
    "450 0.3 0.826036193812 0.840909090909q\\\n",
    "450 0.35 0.836209335219 0.842763719079q\\\n",
    "450 0.4 0.832552693208 0.843199537973q\\\n",
    "450 0.45 0.835051546392 0.846912884127q\\\n",
    "450 0.5 0.840695354802 0.844005641749q\\\n",
    "500 0.1 0.826818830243 0.840413943355q\\\n",
    "500 0.15 0.831778343229 0.844594594595q\\\n",
    "500 0.2 0.833996588971 0.842490842491q\\\n",
    "500 0.25 0.827080394922 0.844720496894q\\\n",
    "500 0.3 0.825239895318 0.846240179574q\\\n",
    "500 0.35 0.834807417974 0.841536273115q\\\n",
    "500 0.4 0.833765485451 0.845046570703q\\\n",
    "500 0.45 0.832904884319 0.843359818388q\\\n",
    "500 0.5 0.828855140187 0.843714609287q\\\n",
    "550 0.1 0.827367205543 0.839187705818q\\\n",
    "550 0.15 0.821367030442 0.840859030837q\\\n",
    "550 0.2 0.817883755589 0.839977540707q\\\n",
    "550 0.25 0.829393335232 0.842933408261q\\\n",
    "550 0.3 0.82637875693 0.842105263158q\\\n",
    "550 0.35 0.834508034959 0.843654540405q\\\n",
    "550 0.4 0.8359308586 0.842989084803q\\\n",
    "550 0.45 0.834474885845 0.842970521542q\\\n",
    "550 0.5 0.836646499568 0.844808126411q\\\n",
    "600 0.1 0.820098180768 0.839416058394q\\\n",
    "600 0.15 0.826036866359 0.837524724498q\\\n",
    "600 0.2 0.820303383897 0.839331255313q\\\n",
    "600 0.25 0.818262674177 0.840636338264q\\\n",
    "600 0.3 0.828306264501 0.840915512857q\\\n",
    "600 0.35 0.822136726842 0.842933408261q\\\n",
    "600 0.4 0.830518345952 0.843011947763q\\\n",
    "600 0.45 0.826239067055 0.841477272727q\\\n",
    "600 0.5 0.83013379872 0.848054529963q\\\n",
    "650 0.1 0.525458248473 0.525458248473q\\\n",
    "650 0.15 0.825746593215 0.841355739106q\\\n",
    "650 0.2 0.814127506735 0.839965645577q\\\n",
    "650 0.25 0.830005704507 0.842495015665q\\\n",
    "650 0.3 0.831021794509 0.843153999431q\\\n",
    "650 0.35 0.82802182027 0.841332580463q\\\n",
    "650 0.4 0.828412744811 0.84292901062q\\\n",
    "650 0.45 0.835734870317 0.846000569314q\\\n",
    "650 0.5 0.833285509326 0.842760180995q\\\n",
    "700 0.1 0.585774058577 0.585774058577q\\\n",
    "700 0.15 0.632674853177 0.632674853177q\\\n",
    "700 0.2 0.585774058577 0.585774058577q\\\n",
    "700 0.25 0.831132340545 0.841570177916q\\\n",
    "700 0.3 0.825517040489 0.841716396704q\\\n",
    "700 0.35 0.834807417974 0.840103716508q\\\n",
    "700 0.4 0.83484676504 0.842405243659q\\\n",
    "700 0.45 0.83587237712 0.842731652026q\\\n",
    "700 0.5 0.834064969271 0.842397336293\\\n",
    "\"\n",
    "\n",
    "r = []\n",
    "for val in prev.split('q'):\n",
    "  vals = [float(v) for v in val.split(' ')]\n",
    "  r.append(tuple(vals))\n",
    "\n",
    "three_layer_more_844 = sorted(r, key=lambda v: -v[3])[:28]\n",
    "three_layer_more_844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 0.5 : 0.848054529963\n",
      "250 0.15 : 0.847065462754\n",
      "450 0.45 : 0.846912884127\n",
      "200 0.4 : 0.846622369878\n",
      "200 0.15 : 0.846543001686\n",
      "500 0.3 : 0.846240179574\n",
      "650 0.45 : 0.846000569314\n",
      "350 0.45 : 0.845936794582\n",
      "200 0.2 : 0.845787853345\n",
      "150 0.4 : 0.845431255337\n",
      "300 0.45 : 0.847178905585\n",
      "350 0.4 : 0.845272206304\n",
      "200 0.35 : 0.845261669024\n",
      "200 0.3 : 0.84512645638\n",
      "200 0.45 : 0.845086377797\n",
      "500 0.4 : 0.845046570703\n",
      "550 0.5 : 0.844808126411\n",
      "500 0.25 : 0.844720496894\n",
      "500 0.15 : 0.844594594595\n",
      "50 0.25 : 0.844571428571\n",
      "250 0.4 : 0.844507042254\n",
      "300 0.2 : 0.844444444444\n",
      "300 0.15 : 0.844280860702\n",
      "350 0.5 : 0.844266970907\n",
      "400 0.45 : 0.84406779661\n",
      "400 0.5 : 0.84406779661\n",
      "50 0.15 : 0.844031531532\n",
      "450 0.5 : 0.844005641749\n"
     ]
    }
   ],
   "source": [
    "for t in three_layer_more_844:\n",
    "  neu, dro = int(t[0]), np.float64(t[1])\n",
    "  print neu, t[1], ':',\n",
    "  \n",
    "  model = get_3_layer_model(neu, dro)\n",
    "  model.fit(Xtrain, ytrain, verbose=0, callbacks=[f1stopperCallback()], nb_epoch=200)\n",
    "  model.load_weights('models/tmp_weights.h5')\n",
    "    \n",
    "  pred = model.predict(Xval)\n",
    "  pred[pred < .5] = 0\n",
    "  pred[pred >= .5] = 1\n",
    "  f1_val_best = f1_score(yval, pred, average='micro')\n",
    "\n",
    "  print f1_val_best\n",
    "\n",
    "  models.append((f1_val_best, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848384424192 0.848054529963 0.847178905585 0.847065462754 0.846912884127 0.846622369878 0.846590909091 0.846543001686 0.846543001686 0.846305141894 0.84630404463 0.846240179574 0.84606613455 0.846022727273 0.846000569314 0.845982142857 0.845939933259 0.845936794582 0.845852017937 0.845787853345 0.845464725644 0.845431255337 0.845390070922 0.845319634703 0.845272206304 0.845261669024 0.84524137931 0.84512645638 0.845094180489 0.845086377797 0.845046570703 0.845039018952 0.844998583168 0.844943820225 0.844910688971 0.844895657078 0.844884488449 0.844837421563 0.844808126411 0.844798180785 0.844720496894 0.844668345928 0.844646407271 0.844594594595 0.844581349762 0.844571428571 0.844507042254 0.844457175594 0.844444444444 0.84440706476 0.844406587166 0.844383561644 0.844280860702 0.844266970907 0.844166903207 0.844119302195 0.844078386822 0.84406779661 0.84406779661 0.844031531532 0.844031531532 0.844005641749\n"
     ]
    }
   ],
   "source": [
    "sorted_by_f1val = sorted(models, key=lambda t: -t[0])\n",
    "for s in sorted_by_f1val:\n",
    "  print s[0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_models(n_models):\n",
    "  best_models = []\n",
    "  for s in sorted_by_f1val:\n",
    "    best_models.append(s[1])\n",
    "    if len(best_models) >= n_models:\n",
    "      break\n",
    "  return best_models\n",
    "\n",
    "def predict_ensemble(models, X):\n",
    "  pred = np.zeros((len(models), X.shape[0], 9))\n",
    "  for idx, model in enumerate(models):\n",
    "    pred[idx] = model.predict(X)    \n",
    "  result = pred.mean(axis=0)\n",
    "  assert result.shape[0] == X.shape[0]\n",
    "  assert result.shape[1] == 9\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1s = []\n",
    "\n",
    "for i in range(len(models)):  \n",
    "  pred = predict_ensemble(get_best_models(i + 1), Xval)\n",
    "  pred[pred < .5] = 0\n",
    "  pred[pred > .5] = 1\n",
    "  \n",
    "  f1s.append(f1_score(yval, pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f002b90e190>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAE4CAYAAAATnCTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVIGCMBEREZEqUBAmIiIiUgX/H5RTSRJ3uX2HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f002b9f9610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(models)), f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.85009806668534604,\n",
       " 0.85074208905068605,\n",
       " 0.85033632286995509,\n",
       " 0.84976130300477393,\n",
       " 0.85015462468372216,\n",
       " 0.84887640449438206,\n",
       " 0.84896125772038189,\n",
       " 0.84863802302723945,\n",
       " 0.84896125772038189,\n",
       " 0.8506457046603032]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8506457046603032"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict_ensemble(get_best_models(20), Xval)\n",
    "pred[pred < .5] = 0\n",
    "pred[pred > .5] = 1\n",
    "\n",
    "f1_score(yval, pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003sg</td>\n",
       "      <td>1 2 3 5 6 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00er5</td>\n",
       "      <td>1 2 3 5 6 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00kad</td>\n",
       "      <td>1 2 3 5 6 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00mc6</td>\n",
       "      <td>1 2 5 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00q7x</td>\n",
       "      <td>1 2 4 5 6 7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  business_id       labels\n",
       "0       003sg  1 2 3 5 6 8\n",
       "1       00er5  1 2 3 5 6 8\n",
       "2       00kad  1 2 3 5 6 8\n",
       "3       00mc6      1 2 5 6\n",
       "4       00q7x  1 2 4 5 6 7"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict_ensemble(get_best_models(20), Xtest)\n",
    "\n",
    "pred[pred < .5] = 0\n",
    "pred[pred > .5] = 1\n",
    "labels = mlb.inverse_transform(pred)\n",
    "labels_str = [' '.join(map(str, l)) for l in labels]\n",
    "results = pd.DataFrame({'business_id': dataTest.index, 'labels': pd.Series(labels_str)})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv('results/forth_ensemble_20.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT\n",
    "\n",
    "5,4,7,10,20 models -- no improvements. Max 81.34"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
